% \VignetteIndexEntry{An introduction to mlass}
% \VignetteDepends{ggplot2, methods}
% \VignetteKeywords{Machine Learning}
% \VignettePackage{mlass}

%\SweaveOpts{prefix.string=images/fig}

\documentclass[a4paper]{article}

\usepackage{Sweave}
\usepackage{a4wide}
\usepackage{times}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage{longtable}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[authoryear,round]{natbib}

\textwidth=6.2in
\textheight=8.5in
%\parskip=.3cm
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in

\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textit{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}
\newcommand{\Rcode}[1]{{\texttt{#1}}}
\newtheorem{theorem}{Theorem}[section]

\newcommand{\R}{\textsf{R}}

\newcommand{\term}[1]{\emph{#1}}
\newcommand{\mref}[2]{\htmladdnormallinkfoot{#2}{#1}}


\bibliographystyle{plainnat}

\title{\Rpackage{mlass}: Machine Learning Algorithms}
\author{Guangchuang Yu \\
\\
Jinan University, Guangzhou, China}

\begin{document}

\maketitle

<<options,echo=FALSE>>=
options(width=60)
require(mlass)
@

\section{Linear Regression with one variable}

<<gradDescent>>=
data(ex1data1)
theta <- c(0,0)
linReg <- gradDescent(X, y, theta, alpha=0.01, max.iter=1500)
getTheta(linReg)
@

@
\begin{figure}[htbp]
\begin{center}
<<plotLinearRegression, fig=T>>=
plot(linReg, xlab="Population of City in 10,000s", ylab="Profit in $10,000s")
@
\end{center}
\caption{\label{plotLinearRegression} Linear Regression with One Variable}
\end{figure}

\section{K-Means Clustering Algorithm}

The K-means algorithm is a method to automatically cluster similar
data examples together. Concretely, you are given a training set
$\{x^{(1)},...,x^{(m)}\}$ (where $x^{(i)} \in \mathbb{R^{n}}$), and want
to group the data into a few cohesive clusters.

The intuition behind K-means is an iterative procedure that starts by
guessing the initial centroids, and then refines this guess by
repeately assigning examples to their closest centroids and then
recomputing the centroids based on the assignments.

The K-means algorithm is as follows:
\begin{itemize}
  \item Initialize centroids

    In \Rpackage{mlass}  package, parameter \textit{centers} can be
    set to K, which will initialize K centroids randomly, or user
    specific centroids.

  \item Refines the centroids

    Find closest centroids for each data point.

    Assign each data point to the closest centroid.

    Recompute the centroids based on the assignments.

    Repeat this procedure until it reach \textit{max.iter} (default is 10).

\end{itemize}

The K-means algorithm will always converge to some final set of means
for the centroids. Note that the converged solution may not always be
ideal and depends on the initial setting of the centroids. Therefore,
in practice the K-means algorithm is usually run a few times with
different random initializations. One way to choose between these
different solutions from different random initializations is to choose
the one with the lowest cost function value (distortion).


Case Study:
\begin{itemize}
  \item Example in ML-class \url{http://ml-class.org}.

<<kmeans>>=
data(ex7data2)
initCentroids <- matrix(c(3,3,6,2,8,5), byrow=T,ncol=2)
xx <- kMeans(X, centers=initCentroids)
## accessing result items
xx["clusters"]
xx["centroids"]
@

@
\begin{figure}[htbp]
\begin{center}
<<plotkMeans, fig=T>>=
plot(xx, trace=TRUE, title="Iteration number 10")
@
\end{center}
\caption{\label{plotkMeans} kMeans algorithm for clustering}
\end{figure}

\item IRIS dataset.
<<iris>>=
data(iris)
iris.data=as.matrix(iris[,-5])
x=kMeans(iris.data, centers=3)
l <- sapply(1:3, function(i)
            names(which.max(table(iris[x["clusters"] == i,5]))))
## clustering accuracy
sum(as.numeric(factor(iris[,5], levels=l)) == x["clusters"])/length(iris[,5])
@

The \Rfunction{plot}  function fo visualizing the clustering result
only support two features. User can use it to visualize the clustering
result, with only the first two columns in iris data plotted.

\end{itemize}

K means algorithm can apply for image compression. As an example,
please refer to:
\url{http://ygc.name/2011/12/26/image-compression-using-kmeans/}.

\section{Support Vector Machine}

Dataset \textit{ex6data1} can be separated by linear boundary.

<<<svm linear>>=
data(ex6data1)
m <- svmTrain(X,y, C=1, kernelFunction="linearKernel")
head(m["X"])
head(m["y"])
m["w"]
m["b"]
m["alphas"]
@

@
\begin{figure}[htbp]
\begin{center}
<<plotsvmLinear, fig=T>>=
plot(m, X,y, type="linear")
@
\end{center}
\caption{\label{plotsvmLinear} svm algorithm for linear decision boundaries}
\end{figure}





<<<svm nonlinear>>=
data(ex6data2)
model <- svmTrain(X,y, C=1, kernelFunction="gaussianKernel")
head(model["X"])
head(model["y"])
model["w"]
model["b"]
head(model["alphas"])
@

@
\begin{figure}[htbp]
\begin{center}
<<plotsvmnonLinear, fig=T>>=
plot(model, X,y, type="nonlinear")
@
\end{center}
\caption{\label{plotsvmnonLinear} svm algorithm for non-linear decision boundaries}
\end{figure}



\section{Session Information}

The version number of R and packages loaded for generating the vignette were:

\begin{verbatim}
<<echo=FALSE,results=tex>>=
sessionInfo()
@

\end{verbatim}

\end{document}
