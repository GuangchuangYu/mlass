% \VignetteIndexEntry{An introduction to mlass}
% \VignetteDepends{ggplot2, methods}
% \VignetteKeywords{Machine Learning}
% \VignettePackage{mlass}

%\SweaveOpts{prefix.string=images/fig}

\documentclass[a4paper]{article}

\usepackage{Sweave}
\usepackage{a4wide}
\usepackage{times}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage{longtable}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[authoryear,round]{natbib}

\textwidth=6.2in
\textheight=8.5in
%\parskip=.3cm
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in

\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textit{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}
\newcommand{\Rcode}[1]{{\texttt{#1}}}
\newtheorem{theorem}{Theorem}[section]

\newcommand{\R}{\textsf{R}}

\newcommand{\term}[1]{\emph{#1}}
\newcommand{\mref}[2]{\htmladdnormallinkfoot{#2}{#1}}


\bibliographystyle{plainnat}

\title{\Rpackage{mlass}: Machine Learning Algorithms}
\author{Guangchuang Yu \\
\\
Jinan University, Guangzhou, China}

\begin{document}

\maketitle

<<options,echo=FALSE>>=
options(width=60)
require(mlass)
@

\section{Linear Regression with one variable}

<<gradDescent>>=
data(ex1data1)
theta <- c(0,0)
linReg <- gradDescent(X, y, theta, alpha=0.01, max.iter=1500)
getTheta(linReg)
@

@
\begin{figure}[htbp]
\begin{center}
<<plotLinearRegression, fig=T>>=
plot(linReg, xlab="Population of City in 10,000s", ylab="Profit in $10,000s")
@
\end{center}
\caption{\label{plotLinearRegression} Linear Regression with One Variable}
\end{figure}

\section{K-Means Clustering Algorithm}

The K-means algorithm is a method to automatically cluster similar
data examples together. Concretely, you are given a training set
$\{x^{(1)},...,x^{(m)}\}$ (where $x^{(i)} \in \mathbb{R^{n}}$), and want
to group the data into a few cohesive clusters.

The intuition behind K-means is an iterative procedure that starts by
guessing the initial centroids, and then refines this guess by
repeately assigning examples to their closest centroids and then
recomputing the centroids based on the assignments.

The K-means algorithm is as follows:
\begin{itemize}
  \item Initialize centroids

    In \Rpackage{mlass}  package, parameter \textit{centers} can be
    set to K, which will initialize K centroids randomly, or user
    specific centroids.

  \item Refines the centroids

    Find closest centroids for each data point.

    Assign each data point to the closest centroid.

    Recompute the centroids based on the assignments.

    Repeat this procedure until it reach \textit{max.iter} (default is 10).

\end{itemize}

The K-means algorithm will always converge to some final set of means
for the centroids. Note that the converged solution may not always be
ideal and depends on the initial setting of the centroids. Therefore,
in practice the K-means algorithm is usually run a few times with
different random initializations. One way to choose between these
different solutions from different random initializations is to choose
the one with the lowest cost function value (distortion).


Case Study:
\begin{itemize}
  \item Example in ML-class \url{http://ml-class.org}.

<<kmeans>>=
data(ex7data2)
initCentroids <- matrix(c(3,3,6,2,8,5), byrow=T,ncol=2)
xx <- kMeans(X, centers=initCentroids)
## accessing result items
xx["clusters"]
xx["centroids"]
@

@
\begin{figure}[htbp]
\begin{center}
<<plotkMeans, fig=T>>=
plot(xx, trace=TRUE, title="Iteration number 10")
@
\end{center}
\caption{\label{plotkMeans} kMeans algorithm for clustering}
\end{figure}

\item IRIS dataset.
<<iris>>=
data(iris)
iris.data=as.matrix(iris[,-5])
x=kMeans(iris.data, centers=3)
l <- sapply(1:3, function(i)
            names(which.max(table(iris[x["clusters"] == i,5]))))
## clustering accuracy
sum(as.numeric(factor(iris[,5], levels=l)) == x["clusters"])/length(iris[,5])
@

The \Rfunction{plot}  function fo visualizing the clustering result
only support two features. User can use it to visualize the clustering
result, with only the first two columns in iris data plotted.

\end{itemize}

K means algorithm can apply for image compression. As an example,
please refer to:
\url{http://ygc.name/2011/12/26/image-compression-using-kmeans/}.

\section{Support Vector Machine}

Vladimir Vapnik invented Support Vector Machines in 1979. In its
simplest form, an SVM is a hyperplane that separates a set of positive
examples from a set of negative examples with maximum margin.

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=.7\linewidth]{linearSVM.png}
    \caption{A linear Support Vector Machine}
    \label{Fig:linearSVM}
  \end{center}
\end{figure}

In the linear case, the margin is defined by the distance of the
\textit{hyperplane} (decision boundary ) to the nearest of the
positive and negative examples, which were also known as
\textit{support vectors}.

The object of SVM is to maximize the distance from the
\textit{hyperplane} to the \textit{support vectors}.

The formula for the output of a linear SVM is

$w^{T}x+b$,

where \textit{w} is the normal vector to the \textit{hyperplane} and
\textit{x} is the input vector. The separating hyperplane is the plane
$u = 0$. The nearest points lie on the planes $u = \pm{1}$. The margin
\textit{m} is thus

$m = \frac{1}{\|\mathbf{w}\|}$

The problem of finding the optimal hyperplane is an optimization
problem and can be solved by optimization techniques (use Lagrange
multipliers to get into a form that can be solved analytically).


Training a support vector machine requires the solution of a very
large quadratic programming (QP) optimization problem. Sequential
Minimal Optimization (SMO) breaks this large QP problem into a series
of smallest possible QP problems. These small QP problems are solved
analytically, which avoids using a time-consuming numerical QP
optimization as an inner loop. The amount of memory required for SMO
is linear in the training set size, which allows SMO to handle very
large training sets.

The SMO algorithm details were describe in \textit{Sequential Minimal
  Optimization: A Fast Algorithm for Training Support Vector Machines}
by John Platt.

\begin{itemize}
  \item finding the maximum margin


\end{itemize}

Case Study:
\begin{itemize}
  \item Linear classification with Example 1 in ML-class.

    Dataset \textit{ex6data1} can be separated by linear boundary.

<<<svm linear>>=
data(ex6data1)
m <- svmTrain(X,y, C=1, kernelFunction="linearKernel")
head(m["X"])
head(m["y"])
m["w"]
m["b"]
m["alphas"]
@

@
\begin{figure}[htbp]
\begin{center}
<<plotsvmLinear, fig=T>>=
plot(m, X,y, type="linear")
@
\end{center}
\caption{\label{plotsvmLinear} svm algorithm for linear decision boundaries}
\end{figure}

Notice that there is an outlier positive example on the far left at
about (0.1, 4.1). In \Rfunction{svmTrain}, the \textit{C} parameter is
a positive value that controls the penalty for misclassified training
examples. A large C parameter tells the SVM to try to classify all the
examples correctly. \textit{C} plays a role similar to
$\frac{1}{\lambda}$, where $\lambda$ is the regularization parameter
for logistic regression.

\item Non-linear classification with Example 2 in ML-class.

  Dataset \textit{ex6data2} is not linearly separable. To find
  non-linear decision boundaries with SVM, we implemented a Gaussian
  kernel, which performed as a similarity function that measures the
  "distance" between a pair of examples, ($x^i, x^j$). The Gaussian
  kernel is also parameterized by a bandwidth parameter, $\sigma$,
  which determines how fast the similarity metric decreases (to 0) as
  the examples are further apart.

<<<svm nonlinear>>=
data(ex6data2)
model <- svmTrain(X,y, C=1, kernelFunction="gaussianKernel")
head(model["X"])
head(model["y"])
model["w"]
model["b"]
head(model["alphas"])
@

@
\begin{figure}[htbp]
\begin{center}
<<plotsvmnonLinear, fig=T>>=
plot(model, X,y, type="nonlinear")
@
\end{center}
\caption{\label{plotsvmnonLinear} svm algorithm for non-linear decision boundaries}
\end{figure}

After using the Gaussian kernel with the SVM, the non-linear decision
boundary was determined reasonably well. The decision boundary is able
to separate most of the positive and negative examples correctly and
follows the contours of the dataset well.

\end{itemize}

\section{Session Information}

The version number of R and packages loaded for generating the vignette were:

\begin{verbatim}
<<echo=FALSE,results=tex>>=
sessionInfo()
@

\end{verbatim}

\end{document}
